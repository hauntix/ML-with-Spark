{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with structured data\n",
    "- Extends RDD to a \"DataFrame\" object\n",
    "- DataFrames:\n",
    "    - Contain Row objects\n",
    "    - Can run SQL queries\n",
    "    - Has a schema (leading to more effecient storage)\n",
    "    - Read and write to JSON, Hive, parquet\n",
    "    - Communicates with JDBC/ODBC, Tableau\n",
    "    \n",
    "## Using Spark SQL in python\n",
    "- `from pyspark.sql import SQLContext, Row`\n",
    "- `hiveContext = HiveContext(sc)`\n",
    "- `inputData = spark.read.json(dataFile)`\n",
    "- `inputData.createOrReplaceTempView(\"mySctructuredStuff\")`\n",
    "- `myResultDataFrame = hiveContext.sql(\"SELECT foo FROM bar ORDER BY foobar\")`\n",
    "\n",
    "## More stuff you can do with DataFrames\n",
    "- `myResultDataFrame.show()`\n",
    "- `myResultDataFrame.select(\"someFieldName\")`\n",
    "- `myResultDataFrame.filter(myResultDataFrame(\"someFieldName\" > 200))`\n",
    "- `myResultDataFrame.groupBy(myResultDataFrame(\"someFieldName\")).mean()`\n",
    "- `myResultDataFrame.rdd().map(mapperFunction)`\n",
    "\n",
    "## DataSets\n",
    "- In Spark 2.0, a DataFrame is really a DataSet of row objects\n",
    "- DataSets can wrap known, typed data too. Butt this is mostly transparent to you in oython, since python is untyped.\n",
    "- So - don't sweat this too much with Python. but the Spark 2.0 way is to use DataSets instead of DataFrames when you can.\n",
    "\n",
    "## Shell Access\n",
    "- Spark SQL exposes a JDBC/ODBC server (if you built Spark with Hive support)\n",
    "- Start it with sbin/start-thriftserver.sh\n",
    "- Listens on port 10000 by default\n",
    "- Connect using bin/beeline -u jdbc:hive2://localhost:10000\n",
    "- Viola, you have a SQL shell to Spark SQL\n",
    "- You can create new tables, or query existing ones that were coached using `hiveCtx.cacheTable(\"tableName\")`\n",
    "\n",
    "## UDF'S - User Defined Functions\n",
    "`\n",
    "from pyspark.sql.types import IntegerType\n",
    "hiveCtx.registerFunction(\"square\", lambda x: x*x, IntegerType())\n",
    "df = hiveCtx.sql(\"SELECT square('someNumericalField') FROM tableName)\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark-SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "\n",
    "import collections\n",
    "\n",
    "# Create a SparkSession (Note, the config section is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"../data/temp\").appName(\"SparkSQL\").getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row(ID=21, age=19, name=\"b'Miles'\", numFriends=268)\n",
      "Row(ID=52, age=19, name=\"b'Beverly'\", numFriends=269)\n",
      "Row(ID=54, age=19, name=\"b'Brunt'\", numFriends=5)\n",
      "Row(ID=106, age=18, name=\"b'Beverly'\", numFriends=499)\n",
      "Row(ID=115, age=18, name=\"b'Dukat'\", numFriends=397)\n",
      "Row(ID=133, age=19, name=\"b'Quark'\", numFriends=265)\n",
      "Row(ID=136, age=19, name=\"b'Will'\", numFriends=335)\n",
      "Row(ID=225, age=19, name=\"b'Elim'\", numFriends=106)\n",
      "Row(ID=304, age=19, name=\"b'Will'\", numFriends=404)\n",
      "Row(ID=341, age=18, name=\"b'Data'\", numFriends=326)\n",
      "Row(ID=366, age=19, name=\"b'Keiko'\", numFriends=119)\n",
      "Row(ID=373, age=19, name=\"b'Quark'\", numFriends=272)\n",
      "Row(ID=377, age=18, name=\"b'Beverly'\", numFriends=418)\n",
      "Row(ID=404, age=18, name=\"b'Kasidy'\", numFriends=24)\n",
      "Row(ID=409, age=19, name=\"b'Nog'\", numFriends=267)\n",
      "Row(ID=439, age=18, name=\"b'Data'\", numFriends=417)\n",
      "Row(ID=444, age=18, name=\"b'Keiko'\", numFriends=472)\n",
      "Row(ID=492, age=19, name=\"b'Dukat'\", numFriends=36)\n",
      "Row(ID=494, age=18, name=\"b'Kasidy'\", numFriends=194)\n",
      "+---+-----+\n",
      "|age|count|\n",
      "+---+-----+\n",
      "| 18|    8|\n",
      "| 19|   11|\n",
      "| 20|    5|\n",
      "| 21|    8|\n",
      "| 22|    7|\n",
      "| 23|   10|\n",
      "| 24|    5|\n",
      "| 25|   11|\n",
      "| 26|   17|\n",
      "| 27|    8|\n",
      "| 28|   10|\n",
      "| 29|   12|\n",
      "| 30|   11|\n",
      "| 31|    8|\n",
      "| 32|   11|\n",
      "| 33|   12|\n",
      "| 34|    6|\n",
      "| 35|    8|\n",
      "| 36|   10|\n",
      "| 37|    9|\n",
      "+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def mapper(line):\n",
    "    fields = line.split(',')\n",
    "    return Row(ID=int(fields[0]), name=str(fields[1].encode(\"utf-8\")), age=int(fields[2]), numFriends=int(fields[3]))\n",
    "\n",
    "lines = spark.sparkContext.textFile(\"../data/fakefriends.csv\")\n",
    "people = lines.map(mapper)\n",
    "\n",
    "# Infer the schema, and register the DataFrame as a table.\n",
    "schemaPeople = spark.createDataFrame(people).cache()\n",
    "schemaPeople.createOrReplaceTempView(\"people\")\n",
    "\n",
    "# SQL can be run over DataFrames that have been registered as a table.\n",
    "teenagers = spark.sql(\"SELECT * FROM people WHERE age >= 13 AND age <= 19\")\n",
    "\n",
    "# The results of SQL queries are RDDs and support all the normal RDD operations.\n",
    "for teen in teenagers.collect():\n",
    "  print(teen)\n",
    "\n",
    "# We can also use functions instead of SQL queries:\n",
    "schemaPeople.groupBy(\"age\").count().orderBy(\"age\").show()\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Movies DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions\n",
    "\n",
    "def loadMovieNames():\n",
    "    movieNames = {}\n",
    "    with open(\"../data/u.item\", encoding=\"ISO-8859-1\") as f:\n",
    "        for line in f:\n",
    "            fields = line.split('|')\n",
    "            movieNames[int(fields[0])] = fields[1]\n",
    "    return movieNames\n",
    "\n",
    "# Create a SparkSession (the config bit is only for Windows!)\n",
    "spark = SparkSession.builder.config(\"spark.sql.warehouse.dir\", \"../data/temp\").appName(\"PopularMovies\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|movieID|count|\n",
      "+-------+-----+\n",
      "|     50|  583|\n",
      "|    258|  509|\n",
      "|    100|  508|\n",
      "|    181|  507|\n",
      "|    294|  485|\n",
      "|    286|  481|\n",
      "|    288|  478|\n",
      "|      1|  452|\n",
      "|    300|  431|\n",
      "|    121|  429|\n",
      "|    174|  420|\n",
      "|    127|  413|\n",
      "|     56|  394|\n",
      "|      7|  392|\n",
      "|     98|  390|\n",
      "|    237|  384|\n",
      "|    117|  378|\n",
      "|    172|  367|\n",
      "|    222|  365|\n",
      "|    313|  350|\n",
      "+-------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "\n",
      "Star Wars (1977): 583\n",
      "Contact (1997): 509\n",
      "Fargo (1996): 508\n",
      "Return of the Jedi (1983): 507\n",
      "Liar Liar (1997): 485\n",
      "English Patient, The (1996): 481\n",
      "Scream (1996): 478\n",
      "Toy Story (1995): 452\n",
      "Air Force One (1997): 431\n",
      "Independence Day (ID4) (1996): 429\n"
     ]
    }
   ],
   "source": [
    "# Load up our movie ID -> name dictionary\n",
    "nameDict = loadMovieNames()\n",
    "\n",
    "# Get the raw data\n",
    "lines = spark.sparkContext.textFile(\"../data/u.data\")\n",
    "# Convert it to a RDD of Row objects\n",
    "movies = lines.map(lambda x: Row(movieID =int(x.split()[1])))\n",
    "# Convert that to a DataFrame\n",
    "movieDataset = spark.createDataFrame(movies)\n",
    "\n",
    "# Some SQL-style magic to sort all movies by popularity in one line!\n",
    "topMovieIDs = movieDataset.groupBy(\"movieID\").count().orderBy(\"count\", ascending=False).cache()\n",
    "\n",
    "# Show the results at this point:\n",
    "\n",
    "#  |movieID|count|\n",
    "#  +-------+-----+\n",
    "#  |     50|  584|\n",
    "#  |    258|  509|\n",
    "#  |    100|  508|\n",
    "#  ...\n",
    "\n",
    "topMovieIDs.show()\n",
    "\n",
    "# Grab the top 10\n",
    "top10 = topMovieIDs.take(10)\n",
    "\n",
    "# Print the results\n",
    "print(\"\\n\")\n",
    "for result in top10:\n",
    "    # Each row has movieID, count as above.\n",
    "    print(\"%s: %d\" % (nameDict[result[0]], result[1]))\n",
    "\n",
    "# Stop the session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
